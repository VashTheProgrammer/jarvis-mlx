# Jarvis MLX ü§ñ

Fine-tuning di modelli esperti AI su Apple Silicon

Sistema per creare e gestire modelli AI specializzati usando **MLX** su Mac (M1/M2/M3/M4).

## Guida Rapida

Leggi **[GUIDA_RAPIDA.md](GUIDA_RAPIDA.md)** per workflow quotidiano (modificare dataset, re-training, test, webapp).

---

## üîß Setup Iniziale (Prima Volta)

```bash
# Attiva ambiente conda
conda activate ml-tuning

# Verifica installazione
cd scripts
python test_setup.py
```

Se mancano dipendenze:
```bash
pip install -r scripts/requirements.txt
```

## üéØ 3 Passi per il Fine-Tuning

### 1Ô∏è‚É£ Crea il Dataset (SUPER SEMPLICE)

Apri `data/my_data.json` e scrivi domande e risposte:

```json
[
  {
    "domanda": "La tua domanda qui",
    "risposta": "La risposta che vuoi insegnare al modello"
  },
  {
    "domanda": "Un'altra domanda",
    "risposta": "Un'altra risposta"
  }
]
```

**√à TUTTO QUI!** Non serve altro formato complicato.

### 2Ô∏è‚É£ Converti il Dataset

```bash
conda activate ml-tuning
cd scripts
python convert_dataset.py ../data/my_data.json
```

Questo crea automaticamente `train.jsonl` e `valid.jsonl` nel formato corretto.

### 3Ô∏è‚É£ Avvia il Training

```bash
python -m mlx_lm lora \
  --model mlx-community/Qwen2.5-7B-Instruct-4bit \
  --train \
  --data ../data \
  --adapter-path ../models/my_model \
  --iters 1000 \
  --batch-size 2 \
  --learning-rate 1e-4
```

**Nota:** Il modello 7B richiede almeno 16GB di RAM.

## üß™ Testa il Modello

**Singola domanda:**
```bash
python -m mlx_lm generate \
  --model mlx-community/Qwen2.5-7B-Instruct-4bit \
  --adapter-path ../models/my_model \
  --prompt "La tua domanda"
```

**Chat interattiva:**
```bash
python -m mlx_lm chat \
  --model mlx-community/Qwen2.5-7B-Instruct-4bit \
  --adapter-path ../models/my_model
```

## üìù Template Dataset

Ecco alcuni template pronti da copiare:

### Q&A Semplice
```json
[
  {"domanda": "Cos'√® X?", "risposta": "X √®..."},
  {"domanda": "Come si fa Y?", "risposta": "Per fare Y..."}
]
```

### Programmazione
```json
[
  {
    "domanda": "Come creare una funzione?",
    "risposta": "def my_function():\n    return 'Hello'"
  }
]
```

### Traduzioni
```json
[
  {"domanda": "Traduci: Ciao", "risposta": "Hello"},
  {"domanda": "Traduci: Grazie", "risposta": "Thank you"}
]
```

### Istruzioni
```json
[
  {
    "domanda": "Scrivi una email formale per richiedere informazioni",
    "risposta": "Gentile [Nome],\n\nLe scrivo per richiedere..."
  }
]
```

## ‚öôÔ∏è Parametri Importanti

| Parametro | Cosa fa | Consigliato |
|-----------|---------|-------------|
| `--iters` | Quanto allena | 500-1000 |
| `--batch-size` | Velocit√† vs RAM | 1-2 |
| `--learning-rate` | Velocit√† apprendimento | 1e-4 |
| `--model` | Modello da usare | Vedi sotto |

## üéØ Modello Utilizzato

```
mlx-community/Qwen2.5-7B-Instruct-4bit
```
- RAM richiesta: ~6-8GB
- Velocit√†: ~0.7 iterazioni/sec
- Qualit√†: Eccellente per la maggior parte dei task
- Ideale per: Conversazione, Q&A, coding, traduzioni

## üí° Tips

1. **Inizia piccolo**: 10-20 esempi bastano per testare
2. **Qualit√† > Quantit√†**: Meglio 50 esempi buoni che 500 mediocri
3. **Monitora il loss**: Deve scendere (es. da 3.0 a < 0.1)
4. **Tempo di training**: ~10-15 minuti per 100 iterazioni con il modello 7B

## üöÄ Workflow Completo

```bash
# 1. Attiva ambiente
conda activate ml-tuning
cd scripts

# 2. Modifica dataset
nano ../data/my_data.json  # o usa VS Code, qualsiasi editor

# 3. Converti
python convert_dataset.py ../data/my_data.json

# 4. Allena
python -m mlx_lm lora \
  --model mlx-community/Qwen2.5-7B-Instruct-4bit \
  --train \
  --data ../data \
  --adapter-path ../models/my_model \
  --iters 1000 \
  --batch-size 2

# 5. Testa
python -m mlx_lm chat \
  --model mlx-community/Qwen2.5-7B-Instruct-4bit \
  --adapter-path ../models/my_model
```

## ‚ùì FAQ

**Q: Quanti esempi servono?**
A: Minimo 10-20, ideale 50-200.

**Q: Quanto tempo ci vuole?**
A: 1000 iterazioni con il 7B = ~2-3 ore (dipende dal Mac).

**Q: Out of memory?**
A: Usa `--batch-size 1` o riduci il numero di iterazioni processate simultaneamente.

**Q: Il modello non impara?**
A: Aumenta `--iters` a 1500-2000 o `--learning-rate` a 5e-4.

**Q: Posso usare formato inglese?**
A: S√¨! Usa `"question"` e `"answer"` invece di `"domanda"` e `"risposta"`.

## üìÅ Struttura Progetto

```
fine-tuning/
‚îú‚îÄ‚îÄ README.md                    # Questa guida
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ my_data.json            # Il tuo dataset (modifica questo!)
‚îÇ   ‚îú‚îÄ‚îÄ train.jsonl             # Generato automaticamente
‚îÇ   ‚îî‚îÄ‚îÄ valid.jsonl             # Generato automaticamente
‚îú‚îÄ‚îÄ models/                      # Adapter LoRA salvati qui
‚îÇ   ‚îî‚îÄ‚îÄ my_model/
‚îÇ       ‚îî‚îÄ‚îÄ adapters.safetensors
‚îî‚îÄ‚îÄ scripts/
    ‚îú‚îÄ‚îÄ requirements.txt         # Dipendenze
    ‚îú‚îÄ‚îÄ test_setup.py           # Test installazione
    ‚îî‚îÄ‚îÄ convert_dataset.py      # Conversione dataset
```

## üåê Web Chat Interface

Chatta con i tuoi modelli tramite interfaccia web!

```bash
cd webapp
./start_server.sh
```

Poi apri il browser su `http://localhost:5000`

### Caratteristiche
- ‚úÖ Interfaccia moderna stile ChatGPT
- ‚úÖ Selezione tra pi√π modelli esperti
- ‚úÖ Accessibile da rete locale (smartphone, tablet, ecc.)
- ‚úÖ Cronologia conversazione
- ‚úÖ Dark mode

Vedi [webapp/README.md](webapp/README.md) per dettagli.

## üéì Creare Modelli Esperti

Crea modelli specializzati in diversi argomenti!

```bash
cd scripts
./create_expert.sh programming_expert 1000
```

Lo script:
1. Crea template dataset
2. (Tu modifichi il dataset con le tue Q&A)
3. Converte e fa training automaticamente
4. Testa il modello

Poi abilita il modello in `models/models_config.json` e sar√† disponibile nella webapp!

**Esempi di esperti disponibili:**
- üíª **Programmazione**: Python, coding, best practices
- ‚≠ê **Astrologia**: Oroscopi, segni zodiacali, carta natale
- üß¨ **Biologia**: Anatomia, scienze naturali
- üìö **Storia**: Eventi storici, personaggi famosi
- üë®‚Äçüç≥ **Cucina**: Ricette italiane, tecniche culinarie
- üìñ **Physophia**: Esperto del mondo fantasy (esempio di training da libro)
- üíº **Il tuo esperto personalizzato!**

### Training da Libro/Documento

Per trainare su un libro o documento lungo (come Physophia), usa il training incrementale:

```bash
# 1. Crea dataset con chunk piccoli (200 parole max)
python3 scripts/create_small_chunks_dataset.py

# 2. Training automatico in batch
scripts/train_all_batches.sh
```

**Nota**: Il fine-tuning su testi narrativi lunghi ha limiti. Per migliori risultati, considera **RAG (Retrieval-Augmented Generation)** invece del fine-tuning diretto.

---

**Tutto qui!** Semplice, no? üöÄ
